{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45727ce4",
   "metadata": {},
   "source": [
    "### Обучение лоры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d05b4050",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/i.danilov/sandbox/nn_labs/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2026-01-10 19:19:55.703597: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/i.danilov/sandbox/nn_labs/venv/lib/python3.11/site-packages/keras/src/export/tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at intfloat/multilingual-e5-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2026-01-10 19:19:59,766 - src.logger - 92 - INFO - XLMRobertaForSequenceClassification(\n",
      "  (roberta): XLMRobertaModel(\n",
      "    (embeddings): XLMRobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): XLMRobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x XLMRobertaLayer(\n",
      "          (attention): XLMRobertaAttention(\n",
      "            (self): XLMRobertaSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): XLMRobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): XLMRobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): XLMRobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): XLMRobertaClassificationHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (out_proj): Linear(in_features=768, out_features=6, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,037,574 || all params: 279,085,836 || trainable%: 0.3718\n"
     ]
    }
   ],
   "source": [
    "from src.lora import LoRAFinetuner\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "DATASET_NAME = \"JTF\"\n",
    "train_df = pd.read_csv(f'data/{DATASET_NAME}/train/df_1_level.csv')\n",
    "val_df = pd.read_csv(f'data/{DATASET_NAME}/val/df_1_level.csv')\n",
    "\n",
    "finetuner = LoRAFinetuner(\n",
    "    model_name=\"intfloat/multilingual-e5-base\",\n",
    "    dataframe=train_df,\n",
    "    validation_dataframe=val_df,\n",
    "    num_labels=6,\n",
    "    target_modules=[\"query\", \"key\", \"value\"],\n",
    "    modules_to_save =['classifier'],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "350db836",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-10 19:19:59,844 - src.logger - 205 - INFO - Частота классов: {0: 701, 1: 792, 2: 1159, 3: 942, 4: 1729, 5: 1116}\n",
      "2026-01-10 19:19:59,847 - src.logger - 210 - INFO - Веса классов: tensor([1.5309, 1.3550, 0.9259, 1.1392, 0.6207, 0.9616])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 6439/6439 [00:05<00:00, 1197.13 examples/s]\n",
      "Map: 100%|██████████| 6439/6439 [00:05<00:00, 1198.60 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='636' max='1060' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 636/1060 12:42 < 08:30, 0.83 it/s, Epoch 12/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.495300</td>\n",
       "      <td>0.386141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.263800</td>\n",
       "      <td>0.171128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.149000</td>\n",
       "      <td>0.125967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.124200</td>\n",
       "      <td>0.116480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.107400</td>\n",
       "      <td>0.113832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.098300</td>\n",
       "      <td>0.113682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.089400</td>\n",
       "      <td>0.112322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.081700</td>\n",
       "      <td>0.113694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.074700</td>\n",
       "      <td>0.113414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.067500</td>\n",
       "      <td>0.117628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.065300</td>\n",
       "      <td>0.117136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.059200</td>\n",
       "      <td>0.121569</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "finetuner.train(batch_size=120, epochs=20, learning_rate=3e-4, early_stopping_patience=5, early_stopping_threshold=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c36f6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<bound method LoRAFinetuner._tokenize_function of <src.lora.LoRAFinetuner object at 0x7f10bd639210>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only shown once. Subsequent hashing failures won't be shown.\n",
      "Map: 100%|██████████| 30048/30048 [00:26<00:00, 1147.76 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8284411608093717,\n",
       " 'hamming_loss': 0.04696374689385872,\n",
       " 'jaccard_micro': 0.7485821183597113,\n",
       " 'jaccard_macro': 0.7586165675101596,\n",
       " 'jaccard_samples': 0.833716054313099,\n",
       " 'precision_micro': 0.8741634592045494,\n",
       " 'recall_micro': 0.8389909478168264,\n",
       " 'f1_micro': 0.8562161427819382,\n",
       " 'precision_macro': 0.8713167811588809,\n",
       " 'recall_macro': 0.8578497586003476,\n",
       " 'f1_macro': 0.8620581352960968}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetuner.evaluate_model(f'data/{DATASET_NAME}/test/df_1_level.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
