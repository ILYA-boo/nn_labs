{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c911b5ee",
   "metadata": {},
   "source": [
    "## В рамках этой ЛР будем использовать другой датасет - собаки против змей. Посмотрим размер картинок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "478d1c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 256\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "image = Image.open(\"labs_data/lab1_2_6/1. Satellite - cloudy vs desert/cloudy/train_12.jpg\")\n",
    "print(image.width, image.height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1760213f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/i.danilov/sandbox/nn_labs/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2026-01-11 09:31:50.622867: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/i.danilov/sandbox/nn_labs/venv/lib/python3.11/site-packages/keras/src/export/tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "from src.logger import logger\n",
    "import torchvision.models as models\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data_dir = \"labs_data/lab1_2_6/2. Animals - dogs vs snakes\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759a0452",
   "metadata": {},
   "source": [
    "### Сделаем трансформации и создадим датасеты для CNN. Данные будут раздеделены train:val:test как 0.6:0.2:0.2\n",
    "#### Применим аугментацию для обучащего набора - случайные повороты, отражения, разные варианты яркости, контрастности и т.д.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4fb0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-11 09:31:53,073 - src.logger - 22 - INFO - Общее количество изображений: 2000\n",
      "2026-01-11 09:31:53,074 - src.logger - 23 - INFO - Классы: ['dogs', 'snakes']\n",
      "2026-01-11 09:31:53,074 - src.logger - 26 - INFO - Класс -> Индекс: {'dogs': 0, 'snakes': 1}\n",
      "2026-01-11 09:31:53,075 - src.logger - 37 - INFO - Размеры наборов: Train=1200, Val=400, Test=400\n",
      "2026-01-11 09:31:53,076 - src.logger - 64 - INFO - Даталоадеры для CNN созданы.\n"
     ]
    }
   ],
   "source": [
    "IMG_SIZE = 256\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_transform_cnn = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), \n",
    "])\n",
    "\n",
    "val_test_transform_cnn = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "full_dataset = datasets.ImageFolder(root=data_dir, transform=None) \n",
    "\n",
    "logger.info(f\"Общее количество изображений: {len(full_dataset)}\")\n",
    "logger.info(f\"Классы: {full_dataset.classes}\")\n",
    "class_to_idx = full_dataset.class_to_idx\n",
    "idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
    "logger.info(f\"Класс -> Индекс: {class_to_idx}\")\n",
    "\n",
    "train_ratio = 0.6\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.2\n",
    "\n",
    "total_size = len(full_dataset)\n",
    "train_size = int(train_ratio * total_size)\n",
    "val_size = int(val_ratio * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "logger.info(f\"Размеры наборов: Train={train_size}, Val={val_size}, Test={test_size}\")\n",
    "\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size], generator=generator)\n",
    "\n",
    "class TransformedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.subset[index]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "train_dataset_cnn = TransformedDataset(train_dataset, transform=train_transform_cnn)\n",
    "val_dataset_cnn = TransformedDataset(val_dataset, transform=val_test_transform_cnn)\n",
    "test_dataset_cnn = TransformedDataset(test_dataset, transform=val_test_transform_cnn)\n",
    "\n",
    "train_loader_cnn = DataLoader(train_dataset_cnn, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader_cnn = DataLoader(val_dataset_cnn, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "test_loader_cnn = DataLoader(test_dataset_cnn, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "logger.info(\"Даталоадеры для CNN созданы.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7caa5e",
   "metadata": {},
   "source": [
    "### Создаим модель на базе resnet50 из torchvision, заморозим веса у предобученной части, а к последнему слою добавим Dropout и BatchNorm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e7194f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/i.danilov/sandbox/nn_labs/venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "2026-01-11 09:31:53,487 - src.logger - 32 - INFO - CNNDogSnakeClassifier(\n",
      "  (backbone): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Sequential(\n",
      "      (0): Dropout(p=0.5, inplace=False)\n",
      "      (1): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): Dropout(p=0.3, inplace=False)\n",
      "      (5): Linear(in_features=512, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class CNNDogSnakeClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(CNNDogSnakeClassifier, self).__init__()\n",
    "        self.backbone = models.resnet50(weights=True)\n",
    "        \n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        num_features = self.backbone.fc.in_features\n",
    "\n",
    "        self.backbone.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "model_cnn = CNNDogSnakeClassifier(num_classes=len(class_to_idx)).to(device)\n",
    "\n",
    "criterion_cnn = nn.CrossEntropyLoss()\n",
    "optimizer_cnn = optim.Adam(model_cnn.parameters(), lr=2e-5)\n",
    "scheduler_cnn = optim.lr_scheduler.StepLR(optimizer_cnn, step_size=7, gamma=0.1)\n",
    "\n",
    "logger.info(model_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4d34d1",
   "metadata": {},
   "source": [
    "### Обучим CNN модель на 10 эпохах, также с методом ранней остановки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67b0322",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-11 09:31:53,498 - src.logger - 7 - INFO - Начинается обучение CNN...\n",
      "2026-01-11 09:31:53,499 - src.logger - 9 - INFO - \n",
      "Epoch 1/10\n",
      "2026-01-11 09:31:53,949 - src.logger - 31 - INFO -   Batch 0, Loss: 0.6957\n",
      "2026-01-11 09:31:55,857 - src.logger - 31 - INFO -   Batch 50, Loss: 0.2899\n",
      "2026-01-11 09:31:56,880 - src.logger - 35 - INFO -   Train Loss: 0.4282, Acc: 0.8408\n",
      "2026-01-11 09:31:57,612 - src.logger - 55 - INFO -   Val Loss: 0.1974, Acc: 0.9900\n",
      "2026-01-11 09:31:57,613 - src.logger - 60 - INFO -   Новая лучшая модель на эпохе 1. Сохранение...\n",
      "2026-01-11 09:31:58,075 - src.logger - 9 - INFO - \n",
      "Epoch 2/10\n",
      "2026-01-11 09:31:58,289 - src.logger - 31 - INFO -   Batch 0, Loss: 0.2381\n",
      "2026-01-11 09:32:00,199 - src.logger - 31 - INFO -   Batch 50, Loss: 0.1262\n",
      "2026-01-11 09:32:01,208 - src.logger - 35 - INFO -   Train Loss: 0.2007, Acc: 0.9750\n",
      "2026-01-11 09:32:01,969 - src.logger - 55 - INFO -   Val Loss: 0.1037, Acc: 0.9975\n",
      "2026-01-11 09:32:01,970 - src.logger - 60 - INFO -   Новая лучшая модель на эпохе 2. Сохранение...\n",
      "2026-01-11 09:32:02,417 - src.logger - 9 - INFO - \n",
      "Epoch 3/10\n",
      "2026-01-11 09:32:02,606 - src.logger - 31 - INFO -   Batch 0, Loss: 0.1680\n",
      "2026-01-11 09:32:04,551 - src.logger - 31 - INFO -   Batch 50, Loss: 0.1577\n",
      "2026-01-11 09:32:05,491 - src.logger - 35 - INFO -   Train Loss: 0.1262, Acc: 0.9875\n",
      "2026-01-11 09:32:06,253 - src.logger - 55 - INFO -   Val Loss: 0.0672, Acc: 0.9975\n",
      "2026-01-11 09:32:06,254 - src.logger - 9 - INFO - \n",
      "Epoch 4/10\n",
      "2026-01-11 09:32:06,448 - src.logger - 31 - INFO -   Batch 0, Loss: 0.0485\n",
      "2026-01-11 09:32:08,442 - src.logger - 31 - INFO -   Batch 50, Loss: 0.1990\n",
      "2026-01-11 09:32:09,431 - src.logger - 35 - INFO -   Train Loss: 0.1154, Acc: 0.9767\n",
      "2026-01-11 09:32:10,203 - src.logger - 55 - INFO -   Val Loss: 0.0500, Acc: 0.9975\n",
      "2026-01-11 09:32:10,204 - src.logger - 9 - INFO - \n",
      "Epoch 5/10\n",
      "2026-01-11 09:32:10,391 - src.logger - 31 - INFO -   Batch 0, Loss: 0.0910\n",
      "2026-01-11 09:32:12,340 - src.logger - 31 - INFO -   Batch 50, Loss: 0.0438\n",
      "2026-01-11 09:32:13,300 - src.logger - 35 - INFO -   Train Loss: 0.0922, Acc: 0.9858\n",
      "2026-01-11 09:32:14,092 - src.logger - 55 - INFO -   Val Loss: 0.0407, Acc: 0.9975\n",
      "2026-01-11 09:32:14,093 - src.logger - 68 - INFO -   Ранняя остановка на эпохе 5\n",
      "2026-01-11 09:32:14,094 - src.logger - 71 - INFO - \n",
      "Обучение CNN завершено.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_EPOCHS_CNN = 10 \n",
    "best_val_acc_cnn = 0.0\n",
    "patience = 3\n",
    "epochs_without_improvement = 3\n",
    "\n",
    "logger.info(\"Начинается обучение CNN...\")\n",
    "for epoch in range(NUM_EPOCHS_CNN):\n",
    "    logger.info(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS_CNN}\")\n",
    "    \n",
    "    model_cnn.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader_cnn):\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "\n",
    "        optimizer_cnn.zero_grad()\n",
    "        outputs = model_cnn(data)\n",
    "        loss = criterion_cnn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer_cnn.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_samples += targets.size(0)\n",
    "        correct_predictions += (predicted == targets).sum().item()\n",
    "\n",
    "        if batch_idx % 50 == 0: \n",
    "            logger.info(f'  Batch {batch_idx}, Loss: {loss.item():.4f}')\n",
    "\n",
    "    epoch_train_loss = running_loss / len(train_loader_cnn)\n",
    "    epoch_train_acc = correct_predictions / total_samples\n",
    "    logger.info(f'  Train Loss: {epoch_train_loss:.4f}, Acc: {epoch_train_acc:.4f}')\n",
    "\n",
    "    model_cnn.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_correct_predictions = 0\n",
    "    val_total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, targets in val_loader_cnn:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            outputs = model_cnn(data)\n",
    "            loss = criterion_cnn(outputs, targets)\n",
    "            \n",
    "            val_running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total_samples += targets.size(0)\n",
    "            val_correct_predictions += (predicted == targets).sum().item()\n",
    "\n",
    "    epoch_val_loss = val_running_loss / len(val_loader_cnn)\n",
    "    epoch_val_acc = val_correct_predictions / val_total_samples\n",
    "    logger.info(f'  Val Loss: {epoch_val_loss:.4f}, Acc: {epoch_val_acc:.4f}')\n",
    "    \n",
    "    scheduler_cnn.step() \n",
    "    \n",
    "    if epoch_val_acc > best_val_acc_cnn:\n",
    "        logger.info(f\"  Новая лучшая модель на эпохе {epoch+1}. Сохранение...\")\n",
    "        best_val_acc_cnn = epoch_val_acc\n",
    "        torch.save(model_cnn.state_dict(), 'best_model_cnn.pth')\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        \n",
    "    if epochs_without_improvement >= patience:\n",
    "        logger.info(f\"  Ранняя остановка на эпохе {epoch+1}\")\n",
    "        break\n",
    "\n",
    "logger.info(\"\\nОбучение CNN завершено.\")\n",
    "model_cnn.load_state_dict(torch.load('best_model_cnn.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73281198",
   "metadata": {},
   "source": [
    "###  Посмотрим точность CNN на тесте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81007fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-11 09:32:14,952 - src.logger - 15 - INFO - \n",
      "Точность модели CNN на тесте: 0.9975\n"
     ]
    }
   ],
   "source": [
    "model_cnn.eval()\n",
    "all_preds_cnn = []\n",
    "all_targets_cnn = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, targets in test_loader_cnn:\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        outputs = model_cnn(data)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        all_preds_cnn.extend(predicted.cpu().numpy())\n",
    "        all_targets_cnn.extend(targets.cpu().numpy())\n",
    "\n",
    "acc_test_cnn = accuracy_score(all_targets_cnn, all_preds_cnn)\n",
    "logger.info(f\"\\nТочность модели CNN на тесте: {acc_test_cnn:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c149aa",
   "metadata": {},
   "source": [
    "### Теперь сделаем трансформации для ViT - здесь помимо аугментаций добавляется изменение размера до 224 на 224, так как именно с такими работает модель, также сформируем датасеты, загрузим преобученную модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f90e9d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-11 09:32:14,964 - src.logger - 23 - INFO - Даталоадеры для ViT созданы.\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2026-01-11 09:32:15,501 - src.logger - 35 - INFO - ViTForImageClassification(\n",
      "  (vit): ViTModel(\n",
      "    (embeddings): ViTEmbeddings(\n",
      "      (patch_embeddings): ViTPatchEmbeddings(\n",
      "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (encoder): ViTEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x ViTLayer(\n",
      "          (attention): ViTAttention(\n",
      "            (attention): ViTSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (output): ViTSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ViTIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ViTOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  )\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5]),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5]),\n",
    "])\n",
    "\n",
    "train_dataset_vit = TransformedDataset(train_dataset, transform=transform)\n",
    "val_dataset_vit = TransformedDataset(val_dataset, transform=transform)\n",
    "test_dataset_vit = TransformedDataset(test_dataset, transform=transform_test)\n",
    "\n",
    "train_loader_vit = DataLoader(train_dataset_vit, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader_vit = DataLoader(val_dataset_vit, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "test_loader_vit = DataLoader(test_dataset_vit, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "logger.info(\"Даталоадеры для ViT созданы.\")\n",
    "\n",
    "model_vit = ViTForImageClassification.from_pretrained(\n",
    "    \"google/vit-base-patch16-224-in21k\",\n",
    "    num_labels=len(class_to_idx),\n",
    "    id2label={i: label for i, label in enumerate(class_to_idx)},\n",
    "    label2id={label: i for i, label in enumerate(class_to_idx)}\n",
    ").to(device)\n",
    "\n",
    "optimizer_vit = optim.Adam(model_vit.parameters(), lr=2e-5)\n",
    "scheduler_vit = optim.lr_scheduler.StepLR(optimizer_vit, step_size=5, gamma=0.8)\n",
    "\n",
    "logger.info(model_vit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629428cd",
   "metadata": {},
   "source": [
    "### Обучим модель с такими же параметрами обучения, как и у CNN модели. В теории для обучения должно понадобится меньшее кол-во эпох (но тут еще вопрос сложности датасета)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71c7fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-11 09:32:15,512 - src.logger - 7 - INFO - \n",
      "Начинается обучение ViT...\n",
      "2026-01-11 09:32:15,513 - src.logger - 10 - INFO - \n",
      "Epoch 1/10\n",
      "2026-01-11 09:32:15,812 - src.logger - 33 - INFO -   Batch 0, Loss: 0.6991\n",
      "2026-01-11 09:32:24,449 - src.logger - 33 - INFO -   Batch 50, Loss: 0.1372\n",
      "2026-01-11 09:32:28,650 - src.logger - 37 - INFO -   Train Loss: 0.2975, Acc: 0.9708\n",
      "2026-01-11 09:32:30,235 - src.logger - 58 - INFO -   Val Loss: 0.0647, Acc: 1.0000\n",
      "2026-01-11 09:32:30,236 - src.logger - 63 - INFO -   Новая лучшая модель ViT на эпохе 1. Сохранение...\n",
      "2026-01-11 09:32:31,724 - src.logger - 10 - INFO - \n",
      "Epoch 2/10\n",
      "2026-01-11 09:32:32,001 - src.logger - 33 - INFO -   Batch 0, Loss: 0.0625\n",
      "2026-01-11 09:32:40,687 - src.logger - 33 - INFO -   Batch 50, Loss: 0.0300\n",
      "2026-01-11 09:32:44,928 - src.logger - 37 - INFO -   Train Loss: 0.0380, Acc: 1.0000\n",
      "2026-01-11 09:32:46,531 - src.logger - 58 - INFO -   Val Loss: 0.0250, Acc: 1.0000\n",
      "2026-01-11 09:32:46,532 - src.logger - 10 - INFO - \n",
      "Epoch 3/10\n",
      "2026-01-11 09:32:46,830 - src.logger - 33 - INFO -   Batch 0, Loss: 0.0239\n",
      "2026-01-11 09:32:55,599 - src.logger - 33 - INFO -   Batch 50, Loss: 0.0160\n",
      "2026-01-11 09:32:59,868 - src.logger - 37 - INFO -   Train Loss: 0.0190, Acc: 1.0000\n",
      "2026-01-11 09:33:01,491 - src.logger - 58 - INFO -   Val Loss: 0.0568, Acc: 0.9825\n",
      "2026-01-11 09:33:01,493 - src.logger - 10 - INFO - \n",
      "Epoch 4/10\n",
      "2026-01-11 09:33:01,795 - src.logger - 33 - INFO -   Batch 0, Loss: 0.0149\n",
      "2026-01-11 09:33:10,609 - src.logger - 33 - INFO -   Batch 50, Loss: 0.0158\n",
      "2026-01-11 09:33:14,891 - src.logger - 37 - INFO -   Train Loss: 0.0245, Acc: 0.9975\n",
      "2026-01-11 09:33:16,508 - src.logger - 58 - INFO -   Val Loss: 0.0147, Acc: 1.0000\n",
      "2026-01-11 09:33:16,510 - src.logger - 71 - INFO -   Ранняя остановка ViT на эпохе 4\n",
      "2026-01-11 09:33:16,510 - src.logger - 74 - INFO - \n",
      "Обучение ViT завершено.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_EPOCHS_VIT = 10 \n",
    "best_val_acc_vit = 0.0\n",
    "patience_vit = 3\n",
    "epochs_without_improvement_vit = 0\n",
    "\n",
    "logger.info(\"\\nНачинается обучение ViT...\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS_VIT):\n",
    "    logger.info(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS_VIT}\")\n",
    "    \n",
    "    model_vit.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader_vit):\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "\n",
    "        optimizer_vit.zero_grad()\n",
    "        outputs = model_vit(pixel_values=data, labels=targets)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer_vit.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        logits = outputs.logits\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        total_samples += targets.size(0)\n",
    "        correct_predictions += (predicted == targets).sum().item()\n",
    "\n",
    "        if batch_idx % 50 == 0:\n",
    "            logger.info(f'  Batch {batch_idx}, Loss: {loss.item():.4f}')\n",
    "\n",
    "    epoch_train_loss = running_loss / len(train_loader_vit)\n",
    "    epoch_train_acc = correct_predictions / total_samples\n",
    "    logger.info(f'  Train Loss: {epoch_train_loss:.4f}, Acc: {epoch_train_acc:.4f}')\n",
    "\n",
    "    model_vit.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_correct_predictions = 0\n",
    "    val_total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, targets in val_loader_vit:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            outputs = model_vit(pixel_values=data, labels=targets)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            val_running_loss += loss.item()\n",
    "            logits = outputs.logits\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            val_total_samples += targets.size(0)\n",
    "            val_correct_predictions += (predicted == targets).sum().item()\n",
    "\n",
    "    epoch_val_loss = val_running_loss / len(val_loader_vit)\n",
    "    epoch_val_acc = val_correct_predictions / val_total_samples\n",
    "    logger.info(f'  Val Loss: {epoch_val_loss:.4f}, Acc: {epoch_val_acc:.4f}')\n",
    "    \n",
    "    scheduler_vit.step()\n",
    "    \n",
    "    if epoch_val_acc > best_val_acc_vit:\n",
    "        logger.info(f\"  Новая лучшая модель ViT на эпохе {epoch+1}. Сохранение...\")\n",
    "        best_val_acc_vit = epoch_val_acc\n",
    "        torch.save(model_vit.state_dict(), 'best_model_vit.pth')\n",
    "        epochs_without_improvement_vit = 0\n",
    "    else:\n",
    "        epochs_without_improvement_vit += 1\n",
    "        \n",
    "    if epochs_without_improvement_vit >= patience_vit:\n",
    "        logger.info(f\"  Ранняя остановка ViT на эпохе {epoch+1}\")\n",
    "        break\n",
    "\n",
    "logger.info(\"\\nОбучение ViT завершено.\")\n",
    "model_vit.load_state_dict(torch.load('best_model_vit.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefdd3cc",
   "metadata": {},
   "source": [
    "### Оценим модель ViT на тесте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87b729fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-11 09:33:18,345 - src.logger - 16 - INFO - \n",
      "Точность модели ViT на тесте: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model_vit.eval()\n",
    "all_preds_vit = []\n",
    "all_targets_vit = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, targets in test_loader_vit:\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        outputs = model_vit(pixel_values=data)\n",
    "        logits = outputs.logits\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        \n",
    "        all_preds_vit.extend(predicted.cpu().numpy())\n",
    "        all_targets_vit.extend(targets.cpu().numpy())\n",
    "\n",
    "acc_test_vit = accuracy_score(all_targets_vit, all_preds_vit)\n",
    "logger.info(f\"\\nТочность модели ViT на тесте: {acc_test_vit:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903b1fdd",
   "metadata": {},
   "source": [
    "### Сравним точность моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68638bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-11 09:34:02,737 - src.logger - 2 - INFO - \n",
      "--- Сравнение моделей ---\n",
      "2026-01-11 09:34:02,738 - src.logger - 3 - INFO - CNN Model (ResNet50) Test Accuracy: 0.9975\n",
      "2026-01-11 09:34:02,738 - src.logger - 4 - INFO - ViT Model (Base) Test Accuracy: 1.0000\n",
      "2026-01-11 09:34:02,738 - src.logger - 9 - INFO - \n",
      "Модель ViT (Base) показала лучший результат на тестовом наборе.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logger.info(\"\\n--- Сравнение моделей ---\")\n",
    "logger.info(f\"CNN Model (ResNet50) Test Accuracy: {acc_test_cnn:.4f}\")\n",
    "logger.info(f\"ViT Model (Base) Test Accuracy: {acc_test_vit:.4f}\")\n",
    "\n",
    "if acc_test_cnn > acc_test_vit:\n",
    "    logger.info(\"\\nМодель CNN (ResNet50) показала лучший результат на тестовом наборе.\")\n",
    "elif acc_test_vit > acc_test_cnn:\n",
    "    logger.info(\"\\nМодель ViT (Base) показала лучший результат на тестовом наборе.\")\n",
    "else:\n",
    "    logger.info(\"\\nМодели показали одинаковый результат на тестовом наборе.\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "models = ['CNN (ResNet50)', 'ViT (Base)']\n",
    "accuracies = [acc_test_cnn, acc_test_vit]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "bars = plt.bar(models, accuracies, color=['skyblue', 'lightcoral'])\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.title('Сравнение точности CNN и ViT на тестовом наборе', pad=15)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2.0, yval, f'{acc:.4f}', ha='center', va='bottom')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e065b2c",
   "metadata": {},
   "source": [
    "### По итогу ввиду простоты датасета обе модели справились с задачей на ура. Так что, на простых задачах лучше использовать простые CNN модели благодаря их легкости по сравнению с ViT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
