{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Классификация текстов с использованием LSTM в PyTorch\n",
    "\n",
    "В этом ноутбуке мы построим модель классификации текстов на основе LSTM-сетей с использованием PyTorch. \n",
    "Датасет содержит статьи на английском языке из 5 тематических классов. Цель — достичь максимально возможного качества классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импорт необходимых библиотек\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузим словари "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка необходимых данных NLTK\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    \n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка датасета\n",
    "df = pd.read_csv('labs_data/lab3_5/Documents topics (Politics 0, Sport 1, Technology 2, Entertainment 3, Business 4).csv')\n",
    "print(f'Размер датасета: {df.shape}')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверка пропущенных значений\n",
    "print(f'Пропущенных значений в столбце Text: {df[\"Text\"].isnull().sum()}')\n",
    "print(f'Пропущенных значений в столбце Label: {df[\"Label\"].isnull().sum()}')\n",
    "\n",
    "# Удаление строк с пропущенными значениями (если есть)\n",
    "df = df.dropna()\n",
    "print(f'Размер датасета после удаления NaN: {df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Анализ распределения классов\n",
    "class_counts = df['Label'].value_counts().sort_index()\n",
    "print('Распределение классов:')\n",
    "for i, count in enumerate(class_counts):\n",
    "    print(f'Класс {i} ({get_class_name(i)}): {count} примеров')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для получения названия класса по метке\n",
    "def get_class_name(label):\n",
    "    class_names = {\n",
    "        0: 'Политика',\n",
    "        1: 'Спорт',\n",
    "        2: 'Технологии',\n",
    "        3: 'Развлечения',\n",
    "        4: 'Бизнес'\n",
    "    }\n",
    "    return class_names.get(label, f'Неизвестный класс {label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Построение гистограммы распределения классов\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=df, x='Label')\n",
    "plt.title('Распределение классов в датасете')\n",
    "plt.xlabel('Метка класса')\n",
    "plt.ylabel('Количество примеров')\n",
    "\n",
    "# Добавление числовых меток над столбцами\n",
    "for i, v in enumerate(class_counts.values):\n",
    "    plt.text(i, v + 10, str(v), ha='center', va='bottom')\n",
    "    \n",
    "# Настройка подписей оси X\n",
    "plt.xticks(ticks=range(5), labels=[get_class_name(i) for i in range(5)])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Анализ дисбаланса классов\n",
    "print('\\nАнализ дисбаланса классов:')\n",
    "max_class = max(class_counts)\n",
    "min_class = min(class_counts)\n",
    "imbalance_ratio = max_class / min_class\n",
    "print(f'Коэффициент дисбаланса (наибольший/наименьший): {imbalance_ratio:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция предобработки текста\n",
    "def preprocess_text(text):\n",
    "    # Приведение к нижнему регистру\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Удаление специальных символов и цифр\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Токенизация\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Удаление стоп-слов\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Объединение токенов обратно в строку\n",
    "    processed_text = ' '.join(tokens)\n",
    "    \n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предобработка текстовых данных\n",
    "print('Предобработка текстовых данных...')\n",
    "df['processed_text'] = df['Text'].apply(preprocess_text)\n",
    "\n",
    "# Просмотр примеров\n",
    "print('\\nПримеры оригинального и обработанного текста:')\n",
    "for i in range(3):\n",
    "    print(f'Оригинал: {df.iloc[i][\"Text\"][:100]}...')\n",
    "    print(f'Обработано: {df.iloc[i][\"processed_text\"][:100]}...')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определение максимальной длины последовательности\n",
    "text_lengths = [len(text.split()) for text in df['processed_text']]\n",
    "max_length = int(np.percentile(text_lengths, 95))  # Используем 95-й перцентиль\n",
    "print(f'Максимальная длина последовательности (95-й перцентиль): {max_length}')\n",
    "\n",
    "# Гистограмма распределения длин текстов\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(text_lengths, bins=50, edgecolor='black')\n",
    "plt.axvline(max_length, color='red', linestyle='--', label=f'Макс. длина: {max_length}')\n",
    "plt.title('Распределение длин текстов (в словах)')\n",
    "plt.xlabel('Количество слов')\n",
    "plt.ylabel('Частота')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализация токенизатора и обучение на текстах\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['processed_text'])\n",
    "\n",
    "# Размер словаря\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(f'Размер словаря: {vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразование текстов в последовательности и паддинг\n",
    "sequences = tokenizer.texts_to_sequences(df['processed_text'])\n",
    "X = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "y = df['Label'].values\n",
    "\n",
    "print(f'Размер входных данных: {X.shape}')\n",
    "print(f'Размер меток: {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделение данных на обучающую, валидационную и тестовую выборки\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp)\n",
    "\n",
    "print(f'Обучающая выборка: {X_train.shape[0]} примеров')\n",
    "print(f'Валидационная выборка: {X_val.shape[0]} примеров')\n",
    "print(f'Тестовая выборка: {X_test.shape[0]} примеров')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Класс датасета для PyTorch\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = torch.tensor(texts, dtype=torch.long)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание загрузчиков данных\n",
    "batch_size = 32\n",
    "train_dataset = TextDataset(X_train, y_train)\n",
    "val_dataset = TextDataset(X_val, y_val)\n",
    "test_dataset = TextDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определение модели LSTM\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes, num_layers=2, dropout=0.3):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim, \n",
    "            hidden_dim, \n",
    "            num_layers=num_layers, \n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True  # Используем двунаправленную LSTM\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_classes)  # *2 из-за двунаправленности\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, (hidden, _) = self.lstm(embedded)\n",
    "        \n",
    "        # Используем последние скрытые состояния\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        \n",
    "        output = self.dropout(hidden)\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализация модели\n",
    "embedding_dim = 100\n",
    "hidden_dim = 128\n",
    "num_classes = 5\n",
    "num_layers = 2\n",
    "dropout = 0.3\n",
    "\n",
    "model = LSTMClassifier(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    num_classes=num_classes,\n",
    "    num_layers=num_layers,\n",
    "    dropout=dropout\n",
    ")\n",
    "\n",
    "# Перемещение модели на GPU (если доступен)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "print(model)\n",
    "print(f'Количество параметров модели: {sum(p.numel() for p in model.parameters()):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определение функции потерь и оптимизатора\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция обучения модели\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=20):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "    patience = 5\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Этап обучения\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        \n",
    "        for texts, labels in train_loader:\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(texts)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "        \n",
    "        epoch_train_loss = running_loss / len(train_loader)\n",
    "        epoch_train_acc = 100 * correct_train / total_train\n",
    "        \n",
    "        # Этап валидации\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for texts, labels in val_loader:\n",
    "                texts, labels = texts.to(device), labels.to(device)\n",
    "                outputs = model(texts)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_val += labels.size(0)\n",
    "                correct_val += (predicted == labels).sum().item()\n",
    "        \n",
    "        epoch_val_loss = val_running_loss / len(val_loader)\n",
    "        epoch_val_acc = 100 * correct_val / total_val\n",
    "        \n",
    "        train_losses.append(epoch_train_loss)\n",
    "        val_losses.append(epoch_val_loss)\n",
    "        train_accuracies.append(epoch_train_acc)\n",
    "        val_accuracies.append(epoch_val_acc)\n",
    "        \n",
    "        # Обновление скорости обучения\n",
    "        scheduler.step(epoch_val_loss)\n",
    "        \n",
    "        print(f'Эпоха [{epoch+1}/{num_epochs}]')\n",
    "        print(f'  Потери (обучение): {epoch_train_loss:.4f}, Точность (обучение): {epoch_train_acc:.2f}%')\n",
    "        print(f'  Потери (валидация): {epoch_val_loss:.4f}, Точность (валидация): {epoch_val_acc:.2f}%')\n",
    "        \n",
    "        # Ранняя остановка\n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            best_val_loss = epoch_val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'Ранняя остановка на эпохе {epoch+1}')\n",
    "                break\n",
    "    \n",
    "    # Загрузка лучшей модели\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return train_losses, val_losses, train_accuracies, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение модели\n",
    "print('Начало обучения...')\n",
    "train_losses, val_losses, train_accuracies, val_accuracies = train_model(\n",
    "    model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация процесса обучения\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# График потерь\n",
    "ax1.plot(train_losses, label='Обучение', marker='o')\n",
    "ax1.plot(val_losses, label='Валидация', marker='s')\n",
    "ax1.set_title('Динамика функции потерь')\n",
    "ax1.set_xlabel('Эпоха')\n",
    "ax1.set_ylabel('Потери')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# График точности\n",
    "ax2.plot(train_accuracies, label='Обучение', marker='o')\n",
    "ax2.plot(val_accuracies, label='Валидация', marker='s')\n",
    "ax2.set_title('Динамика точности')\n",
    "ax2.set_xlabel('Эпоха')\n",
    "ax2.set_ylabel('Точность (%)')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция оценки модели\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for texts, labels in test_loader:\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            outputs = model(texts)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return np.array(all_preds), np.array(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оценка на тестовой выборке\n",
    "test_preds, test_labels = evaluate_model(model, test_loader)\n",
    "\n",
    "# Точность\n",
    "test_accuracy = accuracy_score(test_labels, test_preds)\n",
    "print(f'Точность на тестовой выборке: {test_accuracy:.4f}')\n",
    "\n",
    "# Отчёт по классификации\n",
    "class_names = [get_class_name(i) for i in range(5)]\n",
    "print('\\nОтчёт по классификации:')\n",
    "print(classification_report(test_labels, test_preds, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Матрица ошибок\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Матрица ошибок')\n",
    "plt.xlabel('Предсказанный класс')\n",
    "plt.ylabel('Истинный класс')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция предсказания для нового текста\n",
    "def predict_text(text, model, tokenizer, max_length):\n",
    "    # Предобработка\n",
    "    processed_text = preprocess_text(text)\n",
    "    \n",
    "    # Преобразование в последовательность\n",
    "    sequence = tokenizer.texts_to_sequences([processed_text])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_length, padding='post', truncating='post')\n",
    "    \n",
    "    # Тензор\n",
    "    tensor = torch.tensor(padded_sequence, dtype=torch.long).to(device)\n",
    "    \n",
    "    # Предсказание\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(tensor)\n",
    "        probabilities = torch.softmax(output, dim=1)\n",
    "        predicted_class = torch.argmax(output, dim=1).item()\n",
    "        confidence = probabilities[0][predicted_class].item()\n",
    "    \n",
    "    return predicted_class, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Примеры предсказаний\n",
    "sample_texts = [\n",
    "    \"Правительство объявило о новых мерах в международной торговле.\",\n",
    "    \"Футбольная команда выиграла чемпионат после напряжённого матча.\",\n",
    "    \"Учёные разработали новый алгоритм для приложений искусственного интеллекта.\",\n",
    "    \"Актёр получил награду за выдающуюся игру в фильме.\",\n",
    "    \"Компания сообщила о высокой прибыли и расширила долю на рынке.\"\n",
    "]\n",
    "\n",
    "print('Примеры предсказаний:')\n",
    "for text in sample_texts:\n",
    "    pred_class, confidence = predict_text(text, model, tokenizer, max_length)\n",
    "    print(f'Текст: \"{text[:50]}...\"')\n",
    "    print(f'Предсказанный класс: {get_class_name(pred_class)} (уверенность: {confidence:.3f})')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция подбора гиперпараметров (опционально)\n",
    "def hyperparameter_tuning():\n",
    "    # Комбинации гиперпараметров\n",
    "    hyperparams = [\n",
    "        {'embedding_dim': 100, 'hidden_dim': 64, 'num_layers': 1, 'dropout': 0.2},\n",
    "        {'embedding_dim': 100, 'hidden_dim': 128, 'num_layers': 2, 'dropout': 0.3},\n",
    "        {'embedding_dim': 150, 'hidden_dim': 128, 'num_layers': 2, 'dropout': 0.3},\n",
    "        {'embedding_dim': 100, 'hidden_dim': 128, 'num_layers': 1, 'dropout': 0.2},\n",
    "        {'embedding_dim': 200, 'hidden_dim': 256, 'num_layers': 2, 'dropout': 0.4},\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, params in enumerate(hyperparams):\n",
    "        print(f'\\nТестируется комбинация гиперпараметров {i+1}: {params}')\n",
    "        \n",
    "        model = LSTMClassifier(\n",
    "            vocab_size=vocab_size,\n",
    "            embedding_dim=params['embedding_dim'],\n",
    "            hidden_dim=params['hidden_dim'],\n",
    "            num_classes=num_classes,\n",
    "            num_layers=params['num_layers'],\n",
    "            dropout=params['dropout']\n",
    "        ).to(device)\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)\n",
    "        \n",
    "        train_losses, val_losses, train_accuracies, val_accuracies = train_model(\n",
    "            model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=15\n",
    "        )\n",
    "        \n",
    "        val_preds, val_labels = evaluate_model(model, val_loader)\n",
    "        val_accuracy = accuracy_score(val_labels, val_preds)\n",
    "        \n",
    "        results.append({\n",
    "            'params': params,\n",
    "            'val_accuracy': val_accuracy,\n",
    "            'final_val_loss': val_losses[-1] if val_losses else float('inf')\n",
    "        })\n",
    "        \n",
    "        print(f'Результат — точность на валидации: {val_accuracy:.4f}, потери: {val_losses[-1]:.4f}')\n",
    "    \n",
    "    best_result = max(results, key=lambda x: x['val_accuracy'])\n",
    "    print(f'\\nЛучшие гиперпараметры: {best_result[\"params\"]}')\n",
    "    print(f'Лучшая точность на валидации: {best_result[\"val_accuracy\"]:.4f}')\n",
    "    \n",
    "    return results, best_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подбор гиперпараметров (раскомментируйте для запуска)\n",
    "# results, best_result = hyperparameter_tuning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Итоговый отчёт\n",
    "print('=== ИТОГИ ===')\n",
    "print(f'Точность на тестовой выборке: {test_accuracy:.4f}')\n",
    "print(f'Размер словаря: {vocab_size}')\n",
    "print(f'Максимальная длина последовательности: {max_length}')\n",
    "print(f'Архитектура модели: LSTM с {num_layers} слоями, {hidden_dim} скрытыми нейронами')\n",
    "print(f'Размерность эмбеддингов: {embedding_dim}')\n",
    "print(f'Вероятность dropout: {dropout}')\n",
    "\n",
    "# Анализ результатов\n",
    "print('\\n=== АНАЛИЗ ===')\n",
    "if test_accuracy > 0.8:\n",
    "    print('Модель достигла высокой точности (>80%), что указывает на хорошее качество')\n",
    "else:\n",
    "    print('Точность модели можно улучшить — рассмотрите дальнейшую оптимизацию')\n",
    "\n",
    "# Проверка переобучения\n",
    "final_train_acc = train_accuracies[-1]\n",
    "final_val_acc = val_accuracies[-1]\n",
    "acc_diff = final_train_acc - final_val_acc\n",
    "\n",
    "if acc_diff > 10:\n",
    "    print('Обнаружено значительное переобучение (точность на обучении сильно выше, чем на валидации)')\n",
    "elif acc_diff > 5:\n",
    "    print('Наблюдается умеренное переобучение')\n",
    "else:\n",
    "    print('Хорошая обобщающая способность, переобучение минимально')\n",
    "\n",
    "print(f'Разрыв между точностью на обучении и валидации: {acc_diff:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
