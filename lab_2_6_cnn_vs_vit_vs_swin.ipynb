{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c911b5ee",
   "metadata": {},
   "source": [
    "## В рамках этой ЛР будем использовать другой датасет - собаки против змей. Посмотрим размер картинок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "478d1c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 256\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "image = Image.open(\"labs_data/lab1_2_6/1. Satellite - cloudy vs desert/cloudy/train_12.jpg\")\n",
    "print(image.width, image.height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1760213f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/i.danilov/sandbox/nn_labs/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2026-01-11 10:07:52.301857: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/i.danilov/sandbox/nn_labs/venv/lib/python3.11/site-packages/keras/src/export/tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from transformers import ViTFeatureExtractor, ViTForImageClassification, SwinForImageClassification, ViTImageProcessor\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "from src.logger import logger\n",
    "import torchvision.models as models\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data_dir = \"labs_data/lab1_2_6/2. Animals - dogs vs snakes\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759a0452",
   "metadata": {},
   "source": [
    "### Сделаем трансформации и создадим датасеты для CNN. Данные будут раздеделены train:val:test как 0.6:0.2:0.2\n",
    "#### Применим аугментацию для обучащего набора - случайные повороты, отражения, разные варианты яркости, контрастности и т.д.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f4fb0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-11 10:07:54,880 - src.logger - 21 - INFO - Общее количество изображений: 2000\n",
      "2026-01-11 10:07:54,881 - src.logger - 22 - INFO - Классы: ['dogs', 'snakes']\n",
      "2026-01-11 10:07:54,881 - src.logger - 25 - INFO - Класс -> Индекс: {'dogs': 0, 'snakes': 1}\n",
      "2026-01-11 10:07:54,881 - src.logger - 36 - INFO - Размеры наборов: Train=1200, Val=400, Test=400\n",
      "2026-01-11 10:07:54,883 - src.logger - 63 - INFO - Даталоадеры для CNN созданы.\n"
     ]
    }
   ],
   "source": [
    "IMG_SIZE = 256\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_transform_cnn = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), \n",
    "])\n",
    "\n",
    "val_test_transform_cnn = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "full_dataset = datasets.ImageFolder(root=data_dir, transform=None) \n",
    "\n",
    "logger.info(f\"Общее количество изображений: {len(full_dataset)}\")\n",
    "logger.info(f\"Классы: {full_dataset.classes}\")\n",
    "class_to_idx = full_dataset.class_to_idx\n",
    "idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
    "logger.info(f\"Класс -> Индекс: {class_to_idx}\")\n",
    "\n",
    "train_ratio = 0.6\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.2\n",
    "\n",
    "total_size = len(full_dataset)\n",
    "train_size = int(train_ratio * total_size)\n",
    "val_size = int(val_ratio * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "logger.info(f\"Размеры наборов: Train={train_size}, Val={val_size}, Test={test_size}\")\n",
    "\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size], generator=generator)\n",
    "\n",
    "class TransformedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.subset[index]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "train_dataset_cnn = TransformedDataset(train_dataset, transform=train_transform_cnn)\n",
    "val_dataset_cnn = TransformedDataset(val_dataset, transform=val_test_transform_cnn)\n",
    "test_dataset_cnn = TransformedDataset(test_dataset, transform=val_test_transform_cnn)\n",
    "\n",
    "train_loader_cnn = DataLoader(train_dataset_cnn, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader_cnn = DataLoader(val_dataset_cnn, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "test_loader_cnn = DataLoader(test_dataset_cnn, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "logger.info(\"Даталоадеры для CNN созданы.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7caa5e",
   "metadata": {},
   "source": [
    "### Создаим модель на базе resnet50 из torchvision, заморозим веса у предобученной части, а к последнему слою добавим Dropout и BatchNorm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1e7194f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/i.danilov/sandbox/nn_labs/venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "2026-01-11 10:07:55,326 - src.logger - 29 - INFO - CNNDogSnakeClassifier(\n",
      "  (backbone): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Sequential(\n",
      "      (0): Dropout(p=0.5, inplace=False)\n",
      "      (1): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): Dropout(p=0.3, inplace=False)\n",
      "      (5): Linear(in_features=512, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class CNNDogSnakeClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(CNNDogSnakeClassifier, self).__init__()\n",
    "        self.backbone = models.resnet50(weights=True)\n",
    "        \n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        num_features = self.backbone.fc.in_features\n",
    "\n",
    "        self.backbone.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "model_cnn = CNNDogSnakeClassifier(num_classes=len(class_to_idx)).to(device)\n",
    "\n",
    "criterion_cnn = nn.CrossEntropyLoss()\n",
    "optimizer_cnn = optim.Adam(model_cnn.parameters(), lr=2e-5)\n",
    "scheduler_cnn = optim.lr_scheduler.StepLR(optimizer_cnn, step_size=7, gamma=0.1)\n",
    "\n",
    "logger.info(model_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4d34d1",
   "metadata": {},
   "source": [
    "### Обучим CNN модель на 5 эпохах для лучшей демонстрации разницы между архитектурами моделей, также с методом ранней остановки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c67b0322",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-11 10:07:55,336 - src.logger - 6 - INFO - Начинается обучение CNN...\n",
      "2026-01-11 10:07:55,337 - src.logger - 8 - INFO - \n",
      "Epoch 1/5\n",
      "2026-01-11 10:07:55,820 - src.logger - 30 - INFO -   Batch 0, Loss: 0.6496\n",
      "2026-01-11 10:07:57,710 - src.logger - 30 - INFO -   Batch 50, Loss: 0.3778\n",
      "2026-01-11 10:07:58,664 - src.logger - 34 - INFO -   Train Loss: 0.4577, Acc: 0.8192\n",
      "2026-01-11 10:07:59,382 - src.logger - 54 - INFO -   Val Loss: 0.2049, Acc: 0.9925\n",
      "2026-01-11 10:07:59,383 - src.logger - 59 - INFO -   Новая лучшая модель на эпохе 1. Сохранение...\n",
      "2026-01-11 10:07:59,846 - src.logger - 8 - INFO - \n",
      "Epoch 2/5\n",
      "2026-01-11 10:08:00,019 - src.logger - 30 - INFO -   Batch 0, Loss: 0.3061\n",
      "2026-01-11 10:08:01,948 - src.logger - 30 - INFO -   Batch 50, Loss: 0.1698\n",
      "2026-01-11 10:08:02,928 - src.logger - 34 - INFO -   Train Loss: 0.2110, Acc: 0.9783\n",
      "2026-01-11 10:08:03,645 - src.logger - 54 - INFO -   Val Loss: 0.1103, Acc: 0.9925\n",
      "2026-01-11 10:08:03,646 - src.logger - 8 - INFO - \n",
      "Epoch 3/5\n",
      "2026-01-11 10:08:03,813 - src.logger - 30 - INFO -   Batch 0, Loss: 0.1193\n",
      "2026-01-11 10:08:05,744 - src.logger - 30 - INFO -   Batch 50, Loss: 0.1323\n",
      "2026-01-11 10:08:06,770 - src.logger - 34 - INFO -   Train Loss: 0.1397, Acc: 0.9808\n",
      "2026-01-11 10:08:07,512 - src.logger - 54 - INFO -   Val Loss: 0.0698, Acc: 0.9975\n",
      "2026-01-11 10:08:07,513 - src.logger - 59 - INFO -   Новая лучшая модель на эпохе 3. Сохранение...\n",
      "2026-01-11 10:08:07,990 - src.logger - 8 - INFO - \n",
      "Epoch 4/5\n",
      "2026-01-11 10:08:08,176 - src.logger - 30 - INFO -   Batch 0, Loss: 0.1034\n",
      "2026-01-11 10:08:10,155 - src.logger - 30 - INFO -   Batch 50, Loss: 0.0617\n",
      "2026-01-11 10:08:11,129 - src.logger - 34 - INFO -   Train Loss: 0.1232, Acc: 0.9733\n",
      "2026-01-11 10:08:11,885 - src.logger - 54 - INFO -   Val Loss: 0.0546, Acc: 0.9975\n",
      "2026-01-11 10:08:11,886 - src.logger - 8 - INFO - \n",
      "Epoch 5/5\n",
      "2026-01-11 10:08:12,083 - src.logger - 30 - INFO -   Batch 0, Loss: 0.0790\n",
      "2026-01-11 10:08:14,006 - src.logger - 30 - INFO -   Batch 50, Loss: 0.0655\n",
      "2026-01-11 10:08:14,997 - src.logger - 34 - INFO -   Train Loss: 0.0838, Acc: 0.9883\n",
      "2026-01-11 10:08:15,743 - src.logger - 54 - INFO -   Val Loss: 0.0410, Acc: 1.0000\n",
      "2026-01-11 10:08:15,745 - src.logger - 59 - INFO -   Новая лучшая модель на эпохе 5. Сохранение...\n",
      "2026-01-11 10:08:16,217 - src.logger - 70 - INFO - \n",
      "Обучение CNN завершено.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_EPOCHS_CNN = 5 \n",
    "best_val_acc_cnn = 0.0\n",
    "patience = 3\n",
    "epochs_without_improvement = 3\n",
    "\n",
    "logger.info(\"Начинается обучение CNN...\")\n",
    "for epoch in range(NUM_EPOCHS_CNN):\n",
    "    logger.info(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS_CNN}\")\n",
    "    \n",
    "    model_cnn.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader_cnn):\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "\n",
    "        optimizer_cnn.zero_grad()\n",
    "        outputs = model_cnn(data)\n",
    "        loss = criterion_cnn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer_cnn.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_samples += targets.size(0)\n",
    "        correct_predictions += (predicted == targets).sum().item()\n",
    "\n",
    "        if batch_idx % 50 == 0: \n",
    "            logger.info(f'  Batch {batch_idx}, Loss: {loss.item():.4f}')\n",
    "\n",
    "    epoch_train_loss = running_loss / len(train_loader_cnn)\n",
    "    epoch_train_acc = correct_predictions / total_samples\n",
    "    logger.info(f'  Train Loss: {epoch_train_loss:.4f}, Acc: {epoch_train_acc:.4f}')\n",
    "\n",
    "    model_cnn.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_correct_predictions = 0\n",
    "    val_total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, targets in val_loader_cnn:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            outputs = model_cnn(data)\n",
    "            loss = criterion_cnn(outputs, targets)\n",
    "            \n",
    "            val_running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total_samples += targets.size(0)\n",
    "            val_correct_predictions += (predicted == targets).sum().item()\n",
    "\n",
    "    epoch_val_loss = val_running_loss / len(val_loader_cnn)\n",
    "    epoch_val_acc = val_correct_predictions / val_total_samples\n",
    "    logger.info(f'  Val Loss: {epoch_val_loss:.4f}, Acc: {epoch_val_acc:.4f}')\n",
    "    \n",
    "    scheduler_cnn.step() \n",
    "    \n",
    "    if epoch_val_acc > best_val_acc_cnn:\n",
    "        logger.info(f\"  Новая лучшая модель на эпохе {epoch+1}. Сохранение...\")\n",
    "        best_val_acc_cnn = epoch_val_acc\n",
    "        torch.save(model_cnn.state_dict(), 'best_model_cnn.pth')\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        \n",
    "    if epochs_without_improvement >= patience:\n",
    "        logger.info(f\"  Ранняя остановка на эпохе {epoch+1}\")\n",
    "        break\n",
    "\n",
    "logger.info(\"\\nОбучение CNN завершено.\")\n",
    "model_cnn.load_state_dict(torch.load('best_model_cnn.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73281198",
   "metadata": {},
   "source": [
    "###  Посмотрим точность CNN на тесте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81007fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-11 10:08:17,052 - src.logger - 15 - INFO - \n",
      "Точность модели CNN на тесте: 0.9975\n"
     ]
    }
   ],
   "source": [
    "model_cnn.eval()\n",
    "all_preds_cnn = []\n",
    "all_targets_cnn = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, targets in test_loader_cnn:\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        outputs = model_cnn(data)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        all_preds_cnn.extend(predicted.cpu().numpy())\n",
    "        all_targets_cnn.extend(targets.cpu().numpy())\n",
    "\n",
    "acc_test_cnn = accuracy_score(all_targets_cnn, all_preds_cnn)\n",
    "logger.info(f\"\\nТочность модели CNN на тесте: {acc_test_cnn:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c149aa",
   "metadata": {},
   "source": [
    "### Теперь сделаем трансформации для ViT - здесь помимо аугментаций добавляется изменение размера до 224 на 224, так как именно с такими работает модель, также сформируем датасеты, загрузим преобученную модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f90e9d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-11 10:08:17,061 - src.logger - 23 - INFO - Даталоадеры для ViT созданы.\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2026-01-11 10:08:17,576 - src.logger - 35 - INFO - ViTForImageClassification(\n",
      "  (vit): ViTModel(\n",
      "    (embeddings): ViTEmbeddings(\n",
      "      (patch_embeddings): ViTPatchEmbeddings(\n",
      "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (encoder): ViTEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x ViTLayer(\n",
      "          (attention): ViTAttention(\n",
      "            (attention): ViTSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (output): ViTSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ViTIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ViTOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  )\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5]),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5]),\n",
    "])\n",
    "\n",
    "train_dataset_vit = TransformedDataset(train_dataset, transform=transform)\n",
    "val_dataset_vit = TransformedDataset(val_dataset, transform=transform)\n",
    "test_dataset_vit = TransformedDataset(test_dataset, transform=transform_test)\n",
    "\n",
    "train_loader_vit = DataLoader(train_dataset_vit, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader_vit = DataLoader(val_dataset_vit, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "test_loader_vit = DataLoader(test_dataset_vit, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "logger.info(\"Даталоадеры для ViT созданы.\")\n",
    "\n",
    "model_vit = ViTForImageClassification.from_pretrained(\n",
    "    \"google/vit-base-patch16-224-in21k\",\n",
    "    num_labels=len(class_to_idx),\n",
    "    id2label={i: label for i, label in enumerate(class_to_idx)},\n",
    "    label2id={label: i for i, label in enumerate(class_to_idx)}\n",
    ").to(device)\n",
    "\n",
    "optimizer_vit = optim.Adam(model_vit.parameters(), lr=2e-5)\n",
    "scheduler_vit = optim.lr_scheduler.StepLR(optimizer_vit, step_size=5, gamma=0.8)\n",
    "\n",
    "logger.info(model_vit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629428cd",
   "metadata": {},
   "source": [
    "### Обучим модель с такими же параметрами обучения, как и у CNN модели. В теории для обучения должно понадобится меньшее кол-во эпох (но тут еще вопрос сложности датасета)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a71c7fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-11 10:08:17,586 - src.logger - 6 - INFO - \n",
      "Начинается обучение ViT...\n",
      "2026-01-11 10:08:17,586 - src.logger - 9 - INFO - \n",
      "Epoch 1/5\n",
      "2026-01-11 10:08:17,877 - src.logger - 32 - INFO -   Batch 0, Loss: 0.6840\n",
      "2026-01-11 10:08:26,554 - src.logger - 32 - INFO -   Batch 50, Loss: 0.1219\n",
      "2026-01-11 10:08:30,773 - src.logger - 36 - INFO -   Train Loss: 0.2974, Acc: 0.9650\n",
      "2026-01-11 10:08:32,357 - src.logger - 57 - INFO -   Val Loss: 0.0615, Acc: 1.0000\n",
      "2026-01-11 10:08:32,358 - src.logger - 62 - INFO -   Новая лучшая модель ViT на эпохе 1. Сохранение...\n",
      "2026-01-11 10:08:33,921 - src.logger - 9 - INFO - \n",
      "Epoch 2/5\n",
      "2026-01-11 10:08:34,206 - src.logger - 32 - INFO -   Batch 0, Loss: 0.0557\n",
      "2026-01-11 10:08:42,991 - src.logger - 32 - INFO -   Batch 50, Loss: 0.0280\n",
      "2026-01-11 10:08:47,263 - src.logger - 36 - INFO -   Train Loss: 0.0366, Acc: 0.9992\n",
      "2026-01-11 10:08:48,879 - src.logger - 57 - INFO -   Val Loss: 0.0243, Acc: 1.0000\n",
      "2026-01-11 10:08:48,880 - src.logger - 9 - INFO - \n",
      "Epoch 3/5\n",
      "2026-01-11 10:08:49,170 - src.logger - 32 - INFO -   Batch 0, Loss: 0.0211\n",
      "2026-01-11 10:08:58,004 - src.logger - 32 - INFO -   Batch 50, Loss: 0.0148\n",
      "2026-01-11 10:09:02,282 - src.logger - 36 - INFO -   Train Loss: 0.0175, Acc: 1.0000\n",
      "2026-01-11 10:09:03,874 - src.logger - 57 - INFO -   Val Loss: 0.0179, Acc: 0.9975\n",
      "2026-01-11 10:09:03,875 - src.logger - 9 - INFO - \n",
      "Epoch 4/5\n",
      "2026-01-11 10:09:04,154 - src.logger - 32 - INFO -   Batch 0, Loss: 0.0136\n",
      "2026-01-11 10:09:12,953 - src.logger - 32 - INFO -   Batch 50, Loss: 0.0116\n",
      "2026-01-11 10:09:17,244 - src.logger - 36 - INFO -   Train Loss: 0.0162, Acc: 0.9983\n",
      "2026-01-11 10:09:18,838 - src.logger - 57 - INFO -   Val Loss: 0.0324, Acc: 0.9900\n",
      "2026-01-11 10:09:18,839 - src.logger - 70 - INFO -   Ранняя остановка ViT на эпохе 4\n",
      "2026-01-11 10:09:18,840 - src.logger - 73 - INFO - \n",
      "Обучение ViT завершено.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_EPOCHS_VIT = 5 \n",
    "best_val_acc_vit = 0.0\n",
    "patience_vit = 3\n",
    "epochs_without_improvement_vit = 0\n",
    "\n",
    "logger.info(\"\\nНачинается обучение ViT...\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS_VIT):\n",
    "    logger.info(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS_VIT}\")\n",
    "    \n",
    "    model_vit.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader_vit):\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "\n",
    "        optimizer_vit.zero_grad()\n",
    "        outputs = model_vit(pixel_values=data, labels=targets)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer_vit.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        logits = outputs.logits\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        total_samples += targets.size(0)\n",
    "        correct_predictions += (predicted == targets).sum().item()\n",
    "\n",
    "        if batch_idx % 50 == 0:\n",
    "            logger.info(f'  Batch {batch_idx}, Loss: {loss.item():.4f}')\n",
    "\n",
    "    epoch_train_loss = running_loss / len(train_loader_vit)\n",
    "    epoch_train_acc = correct_predictions / total_samples\n",
    "    logger.info(f'  Train Loss: {epoch_train_loss:.4f}, Acc: {epoch_train_acc:.4f}')\n",
    "\n",
    "    model_vit.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_correct_predictions = 0\n",
    "    val_total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, targets in val_loader_vit:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            outputs = model_vit(pixel_values=data, labels=targets)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            val_running_loss += loss.item()\n",
    "            logits = outputs.logits\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            val_total_samples += targets.size(0)\n",
    "            val_correct_predictions += (predicted == targets).sum().item()\n",
    "\n",
    "    epoch_val_loss = val_running_loss / len(val_loader_vit)\n",
    "    epoch_val_acc = val_correct_predictions / val_total_samples\n",
    "    logger.info(f'  Val Loss: {epoch_val_loss:.4f}, Acc: {epoch_val_acc:.4f}')\n",
    "    \n",
    "    scheduler_vit.step()\n",
    "    \n",
    "    if epoch_val_acc > best_val_acc_vit:\n",
    "        logger.info(f\"  Новая лучшая модель ViT на эпохе {epoch+1}. Сохранение...\")\n",
    "        best_val_acc_vit = epoch_val_acc\n",
    "        torch.save(model_vit.state_dict(), 'best_model_vit.pth')\n",
    "        epochs_without_improvement_vit = 0\n",
    "    else:\n",
    "        epochs_without_improvement_vit += 1\n",
    "        \n",
    "    if epochs_without_improvement_vit >= patience_vit:\n",
    "        logger.info(f\"  Ранняя остановка ViT на эпохе {epoch+1}\")\n",
    "        break\n",
    "\n",
    "logger.info(\"\\nОбучение ViT завершено.\")\n",
    "model_vit.load_state_dict(torch.load('best_model_vit.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefdd3cc",
   "metadata": {},
   "source": [
    "### Оценим модель ViT на тесте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87b729fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-11 10:09:20,685 - src.logger - 16 - INFO - \n",
      "Точность модели ViT на тесте: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model_vit.eval()\n",
    "all_preds_vit = []\n",
    "all_targets_vit = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, targets in test_loader_vit:\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        outputs = model_vit(pixel_values=data)\n",
    "        logits = outputs.logits\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        \n",
    "        all_preds_vit.extend(predicted.cpu().numpy())\n",
    "        all_targets_vit.extend(targets.cpu().numpy())\n",
    "\n",
    "acc_test_vit = accuracy_score(all_targets_vit, all_preds_vit)\n",
    "logger.info(f\"\\nТочность модели ViT на тесте: {acc_test_vit:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344f0e41",
   "metadata": {},
   "source": [
    "### Теперь займемся моделью на основе SWIN.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96684a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-11 10:09:20,978 - src.logger - 6 - INFO - Swin Config - image_mean: [0.485, 0.456, 0.406], image_std: [0.229, 0.224, 0.225]\n",
      "2026-01-11 10:09:20,980 - src.logger - 34 - INFO - Даталоадеры для Swin созданы.\n",
      "Some weights of SwinForImageClassification were not initialized from the model checkpoint at microsoft/swin-tiny-patch4-window7-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2026-01-11 10:09:21,371 - src.logger - 50 - INFO - SwinForImageClassification(\n",
      "  (swin): SwinModel(\n",
      "    (embeddings): SwinEmbeddings(\n",
      "      (patch_embeddings): SwinPatchEmbeddings(\n",
      "        (projection): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
      "      )\n",
      "      (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (encoder): SwinEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): SwinStage(\n",
      "          (blocks): ModuleList(\n",
      "            (0): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=96, out_features=96, bias=True)\n",
      "                  (key): Linear(in_features=96, out_features=96, bias=True)\n",
      "                  (value): Linear(in_features=96, out_features=96, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=96, out_features=96, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): Identity()\n",
      "              (layernorm_after): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=96, out_features=384, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=384, out_features=96, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=96, out_features=96, bias=True)\n",
      "                  (key): Linear(in_features=96, out_features=96, bias=True)\n",
      "                  (value): Linear(in_features=96, out_features=96, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=96, out_features=96, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.00909090880304575)\n",
      "              (layernorm_after): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=96, out_features=384, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=384, out_features=96, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (downsample): SwinPatchMerging(\n",
      "            (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
      "            (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (1): SwinStage(\n",
      "          (blocks): ModuleList(\n",
      "            (0): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=192, out_features=192, bias=True)\n",
      "                  (key): Linear(in_features=192, out_features=192, bias=True)\n",
      "                  (value): Linear(in_features=192, out_features=192, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=192, out_features=192, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.0181818176060915)\n",
      "              (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=192, out_features=768, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=768, out_features=192, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=192, out_features=192, bias=True)\n",
      "                  (key): Linear(in_features=192, out_features=192, bias=True)\n",
      "                  (value): Linear(in_features=192, out_features=192, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=192, out_features=192, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.027272727340459824)\n",
      "              (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=192, out_features=768, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=768, out_features=192, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (downsample): SwinPatchMerging(\n",
      "            (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
      "            (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (2): SwinStage(\n",
      "          (blocks): ModuleList(\n",
      "            (0): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.036363635212183)\n",
      "              (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.045454543083906174)\n",
      "              (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (2): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.054545458406209946)\n",
      "              (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (3): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.06363636255264282)\n",
      "              (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (4): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.0727272778749466)\n",
      "              (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (5): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (key): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (value): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.08181818574666977)\n",
      "              (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (downsample): SwinPatchMerging(\n",
      "            (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
      "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (3): SwinStage(\n",
      "          (blocks): ModuleList(\n",
      "            (0): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.09090909361839294)\n",
      "              (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): SwinLayer(\n",
      "              (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): SwinAttention(\n",
      "                (self): SwinSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): SwinSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): SwinDropPath(p=0.10000000149011612)\n",
      "              (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): SwinIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): SwinOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (pooler): AdaptiveAvgPool1d(output_size=1)\n",
      "  )\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "swin_model_name = 'microsoft/swin-tiny-patch4-window7-224'\n",
    "\n",
    "\n",
    "from transformers import ViTImageProcessor\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(swin_model_name)\n",
    "logger.info(f\"Swin Config - image_mean: {feature_extractor.image_mean}, image_std: {feature_extractor.image_std}\")\n",
    "\n",
    "swin_normalize = transforms.Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)\n",
    "\n",
    "train_transform_swin = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),    \n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std) \n",
    "])\n",
    "\n",
    "test_transform_swin = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)\n",
    "])\n",
    "\n",
    "train_dataset_swin = TransformedDataset(train_dataset, transform=train_transform_swin)\n",
    "val_dataset_swin = TransformedDataset(val_dataset, transform=test_transform_swin)\n",
    "test_dataset_swin = TransformedDataset(test_dataset, transform=test_transform_swin)\n",
    "\n",
    "train_loader_swin = DataLoader(train_dataset_swin, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader_swin = DataLoader(val_dataset_swin, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "test_loader_swin = DataLoader(test_dataset_swin, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "logger.info(\"Даталоадеры для Swin созданы.\")\n",
    "\n",
    "\n",
    "model_swin = SwinForImageClassification.from_pretrained(swin_model_name, num_labels=2,\n",
    "            ignore_mismatched_sizes=True)\n",
    "model_swin.config.id2label = idx_to_class\n",
    "model_swin.config.label2id = class_to_idx\n",
    "\n",
    "model_swin.to(device)\n",
    "\n",
    "optimizer_swin = optim.Adam(model_swin.parameters(), lr=2e-5)\n",
    "scheduler_swin = optim.lr_scheduler.StepLR(optimizer_swin, step_size=5, gamma=0.8)\n",
    "\n",
    "logger.info(model_swin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e56f6c3",
   "metadata": {},
   "source": [
    "### Обучим модель SWIN с такими же параметрами обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ecf8be9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-11 10:09:21,382 - src.logger - 6 - INFO - \n",
      "Начинается обучение Swin...\n",
      "2026-01-11 10:09:21,383 - src.logger - 9 - INFO - \n",
      "Epoch 1/5\n",
      "2026-01-11 10:09:21,744 - src.logger - 32 - INFO -   Batch 0, Loss: 0.6937\n",
      "2026-01-11 10:09:25,536 - src.logger - 32 - INFO -   Batch 50, Loss: 0.0069\n",
      "2026-01-11 10:09:27,393 - src.logger - 36 - INFO -   Train Loss: 0.1146, Acc: 0.9717\n",
      "2026-01-11 10:09:28,257 - src.logger - 57 - INFO -   Val Loss: 0.0081, Acc: 1.0000\n",
      "2026-01-11 10:09:28,258 - src.logger - 62 - INFO -   Новая лучшая модель Swin на эпохе 1. Сохранение...\n",
      "2026-01-11 10:09:28,815 - src.logger - 9 - INFO - \n",
      "Epoch 2/5\n",
      "2026-01-11 10:09:29,050 - src.logger - 32 - INFO -   Batch 0, Loss: 0.0021\n",
      "2026-01-11 10:09:32,835 - src.logger - 32 - INFO -   Batch 50, Loss: 0.0006\n",
      "2026-01-11 10:09:34,705 - src.logger - 36 - INFO -   Train Loss: 0.0022, Acc: 1.0000\n",
      "2026-01-11 10:09:35,582 - src.logger - 57 - INFO -   Val Loss: 0.0016, Acc: 1.0000\n",
      "2026-01-11 10:09:35,583 - src.logger - 9 - INFO - \n",
      "Epoch 3/5\n",
      "2026-01-11 10:09:35,817 - src.logger - 32 - INFO -   Batch 0, Loss: 0.0001\n",
      "2026-01-11 10:09:39,625 - src.logger - 32 - INFO -   Batch 50, Loss: 0.0002\n",
      "2026-01-11 10:09:41,502 - src.logger - 36 - INFO -   Train Loss: 0.0010, Acc: 1.0000\n",
      "2026-01-11 10:09:42,408 - src.logger - 57 - INFO -   Val Loss: 0.0008, Acc: 1.0000\n",
      "2026-01-11 10:09:42,409 - src.logger - 9 - INFO - \n",
      "Epoch 4/5\n",
      "2026-01-11 10:09:42,667 - src.logger - 32 - INFO -   Batch 0, Loss: 0.0003\n",
      "2026-01-11 10:09:46,446 - src.logger - 32 - INFO -   Batch 50, Loss: 0.0004\n",
      "2026-01-11 10:09:48,320 - src.logger - 36 - INFO -   Train Loss: 0.0005, Acc: 1.0000\n",
      "2026-01-11 10:09:49,224 - src.logger - 57 - INFO -   Val Loss: 0.0015, Acc: 1.0000\n",
      "2026-01-11 10:09:49,225 - src.logger - 70 - INFO -   Ранняя остановка Swin на эпохе 4\n",
      "2026-01-11 10:09:49,226 - src.logger - 73 - INFO - \n",
      "Обучение Swin завершено.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_EPOCHS_SWIN = 5 \n",
    "best_val_acc_swin = 0.0\n",
    "patience_swin = 3\n",
    "epochs_without_improvement_swin = 0\n",
    "\n",
    "logger.info(\"\\nНачинается обучение Swin...\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS_SWIN):\n",
    "    logger.info(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS_SWIN}\")\n",
    "    \n",
    "    model_swin.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader_swin):\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "\n",
    "        optimizer_swin.zero_grad()\n",
    "        outputs = model_swin(pixel_values=data, labels=targets)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer_swin.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        logits = outputs.logits\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        total_samples += targets.size(0)\n",
    "        correct_predictions += (predicted == targets).sum().item()\n",
    "\n",
    "        if batch_idx % 50 == 0:\n",
    "            logger.info(f'  Batch {batch_idx}, Loss: {loss.item():.4f}')\n",
    "\n",
    "    epoch_train_loss = running_loss / len(train_loader_swin)\n",
    "    epoch_train_acc = correct_predictions / total_samples\n",
    "    logger.info(f'  Train Loss: {epoch_train_loss:.4f}, Acc: {epoch_train_acc:.4f}')\n",
    "\n",
    "    model_swin.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_correct_predictions = 0\n",
    "    val_total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, targets in val_loader_swin:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            outputs = model_swin(pixel_values=data, labels=targets)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            val_running_loss += loss.item()\n",
    "            logits = outputs.logits\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            val_total_samples += targets.size(0)\n",
    "            val_correct_predictions += (predicted == targets).sum().item()\n",
    "\n",
    "    epoch_val_loss = val_running_loss / len(val_loader_swin)\n",
    "    epoch_val_acc = val_correct_predictions / val_total_samples\n",
    "    logger.info(f'  Val Loss: {epoch_val_loss:.4f}, Acc: {epoch_val_acc:.4f}')\n",
    "    \n",
    "    scheduler_swin.step()\n",
    "    \n",
    "    if epoch_val_acc > best_val_acc_swin:\n",
    "        logger.info(f\"  Новая лучшая модель Swin на эпохе {epoch+1}. Сохранение...\")\n",
    "        best_val_acc_swin = epoch_val_acc\n",
    "        torch.save(model_swin.state_dict(), 'best_model_swin.pth')\n",
    "        epochs_without_improvement_swin = 0\n",
    "    else:\n",
    "        epochs_without_improvement_swin += 1\n",
    "        \n",
    "    if epochs_without_improvement_swin >= patience_swin:\n",
    "        logger.info(f\"  Ранняя остановка Swin на эпохе {epoch+1}\")\n",
    "        break\n",
    "\n",
    "logger.info(\"\\nОбучение Swin завершено.\")\n",
    "model_swin.load_state_dict(torch.load('best_model_swin.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9ea144",
   "metadata": {},
   "source": [
    "### Оценим модель Swin на тесте\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60d38b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-11 10:09:50,233 - src.logger - 16 - INFO - \n",
      "Точность модели Swin на тесте: 0.9975\n"
     ]
    }
   ],
   "source": [
    "model_swin.eval()\n",
    "all_preds_swin = []\n",
    "all_targets_swin = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, targets in test_loader_swin:\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        outputs = model_swin(pixel_values=data)\n",
    "        logits = outputs.logits\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        \n",
    "        all_preds_swin.extend(predicted.cpu().numpy())\n",
    "        all_targets_swin.extend(targets.cpu().numpy())\n",
    "\n",
    "acc_test_swin = accuracy_score(all_targets_swin, all_preds_swin)\n",
    "logger.info(f\"\\nТочность модели Swin на тесте: {acc_test_swin:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903b1fdd",
   "metadata": {},
   "source": [
    "### Сравним точность моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68638bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-11 10:09:50,247 - src.logger - 1 - INFO - \n",
      "--- Сравнение моделей ---\n",
      "2026-01-11 10:09:50,248 - src.logger - 2 - INFO - CNN Model (ResNet50) Test Accuracy: 0.9975\n",
      "2026-01-11 10:09:50,248 - src.logger - 3 - INFO - ViT Model (Base) Test Accuracy: 1.0000\n",
      "2026-01-11 10:09:50,249 - src.logger - 4 - INFO - Swin Model (Tiny) Test Accuracy: 0.9975\n",
      "2026-01-11 10:09:50,249 - src.logger - 13 - INFO - \n",
      "Лучшая модель: ViT (Base) с точностью 1.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logger.info(\"\\n--- Сравнение моделей ---\")\n",
    "logger.info(f\"CNN Model (ResNet50) Test Accuracy: {acc_test_cnn:.4f}\")\n",
    "logger.info(f\"ViT Model (Base) Test Accuracy: {acc_test_vit:.4f}\")\n",
    "logger.info(f\"Swin Model (Tiny) Test Accuracy: {acc_test_swin:.4f}\")\n",
    "\n",
    "all_accuracies = [acc_test_cnn, acc_test_vit, acc_test_swin]\n",
    "all_models = ['CNN (ResNet50)', 'ViT (Base)', 'Swin (Tiny)']\n",
    "\n",
    "best_model_idx = np.argmax(all_accuracies)\n",
    "best_model_name = all_models[best_model_idx]\n",
    "best_accuracy = all_accuracies[best_model_idx]\n",
    "\n",
    "logger.info(f\"\\nЛучшая модель: {best_model_name} с точностью {best_accuracy:.4f}\")\n",
    "\n",
    "models = all_models\n",
    "accuracies = all_accuracies\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(models, accuracies, color=['skyblue', 'lightcoral', 'lightgreen'])\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.title('Сравнение точности CNN, ViT и Swin на тестовом наборе', pad=15)\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2.0, yval, f'{acc:.4f}', ha='center', va='bottom')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e065b2c",
   "metadata": {},
   "source": [
    "### По итогу ввиду простоты датасета все модели справились с задачей на ура. \n",
    "### Так что, на простых задачах лучше использовать простые CNN модели благодаря их легкости по сравнению с ViT и SWIN и при небольших объемах данных.\n",
    "### А если выбирать между моделями на основе архитектуры трансформера - то SWIN выигрывает благодаря скорости обучения и лучше подойдет при ограниченных ресурсах и лучшему учету локальных признаков, а ViT благодаря лучшей точности на больших объемах данных и работе с глобальными признаками"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
